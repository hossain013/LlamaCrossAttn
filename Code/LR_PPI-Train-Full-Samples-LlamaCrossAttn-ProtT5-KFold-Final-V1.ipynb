{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efac3282-17b1-45e0-ae4a-1034adcf63c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-04 06:50:47.970231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767531048.163085    8582 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767531048.172666    8582 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767531048.455051    8582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767531048.455078    8582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767531048.455081    8582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767531048.455083    8582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] ='tensorflow'\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from keras import layers, Model, Input\n",
    "from keras_hub.src.models.llama.llama_decoder import LlamaTransformerDecoder\n",
    "from keras_hub.src.models.llama.llama_layernorm import LlamaLayerNorm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18607b-2f76-46ab-b443-465dfee810cc",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4023231-5c03-4f31-aa67-504f145b1cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65849, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Protein_1_ID</th>\n",
       "      <th>Protein_2_ID</th>\n",
       "      <th>Protein_1_Seq</th>\n",
       "      <th>Protein_2_Seq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NP_663777.1</td>\n",
       "      <td>NP_001233.1</td>\n",
       "      <td>MESSKKMDSPGALQTNPPLKLHTDRSAGTPVFVPEQGGYKEKFVKT...</td>\n",
       "      <td>MARPHPWWLCVLGTLVGLSATPAPKSCPERHYWAQGKLCCQMCEPG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NP_003630.1</td>\n",
       "      <td>NP_001073594.1</td>\n",
       "      <td>MNRHLWKSQLCEMVQPSGGPAADQDVLGEESPLGKPAMLHLPSEQG...</td>\n",
       "      <td>MEGGRRARVVIESKRNFFLGAFPTPFPAEHVELGRLGDSETAMVPG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NP_001001998.1</td>\n",
       "      <td>NP_067000.1</td>\n",
       "      <td>MAPPSTREPRVLSATSATKSDGEMVLPGFPDADSFVKFALGSVVAV...</td>\n",
       "      <td>MLFYSFFKSLVGKDVVVELKNDLSICGTLHSVDQYLNIKLTDISVT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NP_001033.1</td>\n",
       "      <td>NP_002037.2</td>\n",
       "      <td>MPSGFQQIGSEDGEPPQQRVTGTLVLAVFSAVLGSLQFGYNIGVIN...</td>\n",
       "      <td>MGKVKVGVNGFGRIGRLVTRAAFNSGKVDIVAINDPFIDLNYMVYM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NP_003001.1</td>\n",
       "      <td>NP_006292.2</td>\n",
       "      <td>MAAPSPSGGGGSGGGSGSGTPGPVGSPAPGHPAVSSMQGKRKALKL...</td>\n",
       "      <td>MACLHETRTPSPSFGGFVSTLSEASMRKLDPDTSDCTPEKDLTPTH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index    Protein_1_ID    Protein_2_ID  \\\n",
       "0      1     NP_663777.1     NP_001233.1   \n",
       "2      3     NP_003630.1  NP_001073594.1   \n",
       "3      4  NP_001001998.1     NP_067000.1   \n",
       "5      6     NP_001033.1     NP_002037.2   \n",
       "6      7     NP_003001.1     NP_006292.2   \n",
       "\n",
       "                                       Protein_1_Seq  \\\n",
       "0  MESSKKMDSPGALQTNPPLKLHTDRSAGTPVFVPEQGGYKEKFVKT...   \n",
       "2  MNRHLWKSQLCEMVQPSGGPAADQDVLGEESPLGKPAMLHLPSEQG...   \n",
       "3  MAPPSTREPRVLSATSATKSDGEMVLPGFPDADSFVKFALGSVVAV...   \n",
       "5  MPSGFQQIGSEDGEPPQQRVTGTLVLAVFSAVLGSLQFGYNIGVIN...   \n",
       "6  MAAPSPSGGGGSGGGSGSGTPGPVGSPAPGHPAVSSMQGKRKALKL...   \n",
       "\n",
       "                                       Protein_2_Seq  label  \n",
       "0  MARPHPWWLCVLGTLVGLSATPAPKSCPERHYWAQGKLCCQMCEPG...      0  \n",
       "2  MEGGRRARVVIESKRNFFLGAFPTPFPAEHVELGRLGDSETAMVPG...      0  \n",
       "3  MLFYSFFKSLVGKDVVVELKNDLSICGTLHSVDQYLNIKLTDISVT...      0  \n",
       "5  MGKVKVGVNGFGRIGRLVTRAAFNSGKVDIVAINDPFIDLNYMVYM...      0  \n",
       "6  MACLHETRTPSPSFGGFVSTLSEASMRKLDPDTSDCTPEKDLTPTH...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../ppi/data/LR_PPI.csv\")\n",
    "max_seq = 1500\n",
    "df = df[(df['Protein_1_Seq'].str.len()<=max_seq) & (df['Protein_2_Seq'].str.len()<=max_seq)]# .groupby('label',group_keys =False).apply(lambda x: x.sample(4000,replace=False))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada2f974-5c80-4fb4-8475-30d1063e8e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "file_path = \"../ppi/data/LR_PPI-ProtT5-Full.h5\"\n",
    "\n",
    "loaded_data = {}\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    for seq in hf.keys():\n",
    "        loaded_data[seq] = hf[seq][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cae6a53-8e6e-4f82-96fb-970c6bf9c32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "def process_sequence_tf(x_emb, max_len=512, pad_value=0.0):\n",
    "    seq_len = x_emb.shape[0]\n",
    "    \n",
    "    if seq_len > max_len:\n",
    "        # truncate\n",
    "        x_emb = tf.convert_to_tensor(x_emb[:max_len])\n",
    "        mask = tf.ones([max_len], dtype=tf.float32)\n",
    "    else:\n",
    "        # pad\n",
    "        pad_len = max_len - seq_len\n",
    "        paddings = [[0, pad_len], [0, 0]]\n",
    "        x_emb = tf.pad(x_emb, paddings, constant_values=pad_value)\n",
    "        mask = tf.pad(tf.ones([seq_len], dtype=tf.float32), [[0, pad_len]], constant_values=0.0)\n",
    "    \n",
    "    return x_emb, mask\n",
    "\n",
    "# -----------------------------\n",
    "# Keras Sequence Loader\n",
    "# -----------------------------\n",
    "class DataSequenceLoader(Sequence):\n",
    "    def __init__(self, df, batch_size=32, shuffle=True, max_len=512, pad_value=0.0):\n",
    "        self.prot1_emb = df[\"Protein_1_Seq\"].values\n",
    "        self.prot2_emb = df[\"Protein_2_Seq\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.max_len = max_len\n",
    "        self.pad_value = pad_value\n",
    "        self.indices = np.arange(len(df))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "        x1_list, x2_list, m1_list, m2_list = [], [], [], []\n",
    "        labels_list = []\n",
    "\n",
    "        for i in batch_idx:\n",
    "            emb1 = loaded_data[self.prot1_emb[i]].reshape(-1, 1024)\n",
    "            emb2 = loaded_data[self.prot2_emb[i]].reshape(-1, 1024)\n",
    "\n",
    "            # ---- process embeddings + mask ----\n",
    "            x1_pad, mask1 = process_sequence_tf(emb1, max_len=self.max_len, pad_value=self.pad_value)\n",
    "            x2_pad, mask2 = process_sequence_tf(emb2, max_len=self.max_len, pad_value=self.pad_value)\n",
    "\n",
    "            x1_list.append(x1_pad.numpy())\n",
    "            x2_list.append(x2_pad.numpy())\n",
    "            m1_list.append(mask1.numpy())\n",
    "            m2_list.append(mask2.numpy())\n",
    "            labels_list.append(self.labels[i])\n",
    "\n",
    "        # Convert lists to arrays for batch\n",
    "        x1_batch = np.stack(x1_list, axis=0)\n",
    "        x2_batch = np.stack(x2_list, axis=0)\n",
    "        m1_batch = np.stack(m1_list, axis=0)\n",
    "        m2_batch = np.stack(m2_list, axis=0)\n",
    "        labels_batch = np.array(labels_list)\n",
    "\n",
    "        return (x1_batch, x2_batch, m1_batch, m2_batch), labels_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e053595-ecc9-4960-8d23-7f77235440b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Hybrid Pooling Layer (Max + Avg)\n",
    "# -------------------------------------------------------------------\n",
    "class HybridPooling(layers.Layer):\n",
    "    def call(self, x):\n",
    "        max_pooled = keras.ops.max(x, axis=1)\n",
    "        avg_pooled = keras.ops.mean(x, axis=1)\n",
    "        return keras.ops.concatenate([max_pooled, avg_pooled], axis=-1)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Conv Block (Conv1D → ReLU → Dropout → MaxPool)\n",
    "# -------------------------------------------------------------------\n",
    "def conv_block(x, filters =  100, kernel_sz =20, stride =10, dropout = 0.5):\n",
    "    x = layers.Conv1D(filters, kernel_sz, strides=stride)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=1, padding=\"same\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Cross-attention (Query=X1, Key=X2, Value=X2)\n",
    "# -------------------------------------------------------------------\n",
    "def cross_attention_block(query, key, value, mask, num_heads =4, key_dim =32):\n",
    "    attn = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, \n",
    "        key_dim=key_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        use_bias=True,\n",
    "        # flash_attention=None,  # if GPU supports\n",
    "    )(query, key, value, key_mask=mask, value_mask=mask)\n",
    "    return attn\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# LLaMA Self-Attention Block\n",
    "# -------------------------------------------------------------------\n",
    "def llama_self_attention(x, mask, hidden_dim =100, num_heads = 4):\n",
    "    # single LLaMA decoder layer\n",
    "    llama = LlamaTransformerDecoder(\n",
    "        intermediate_dim=hidden_dim * 4,\n",
    "        # num_heads=num_heads,\n",
    "        num_query_heads=8,\n",
    "        num_key_value_heads=2,\n",
    "        dropout=0.0,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        activation=\"silu\"\n",
    "    )\n",
    "    return llama(x, decoder_padding_mask=mask)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Functional API Model (PyTorch → Keras Conversion)\n",
    "# -------------------------------------------------------------------\n",
    "def build_model(\n",
    "    input_dim=1024,\n",
    "    conv_out=100,\n",
    "    kernel_sz=20,\n",
    "    stride=10,\n",
    "    heads=4,\n",
    "    d_dim=32,\n",
    "    drop_pool=0.5,\n",
    "    drop_linear=0.3\n",
    "):\n",
    "    # Inputs\n",
    "    inp1 = Input((None, input_dim))\n",
    "    inp2 = Input((None, input_dim))\n",
    "    mask1 = Input((None, ))\n",
    "    mask2 = Input((None, ))\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 1) Convolutional features\n",
    "    # ---------------------------------------------------\n",
    "    x1 = layers.Dense(100, use_bias=False,name='stem_conv1')(inp1)\n",
    "    x1 = layers.BatchNormalization(momentum=0.95,name='stem_bn1')(x1)\n",
    "    \n",
    "    x2 = layers.Dense(100, use_bias=False,name='stem_conv2')(inp2)\n",
    "    x2 = layers.BatchNormalization(momentum=0.95,name='stem_bn2')(x2)\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2) Self Attention using LLaMA blocks\n",
    "    # ---------------------------------------------------\n",
    "    s1 = llama_self_attention(x1, mask1, conv_out, heads)\n",
    "    s2 = llama_self_attention(x2, mask2, conv_out, heads)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 3) Cross Attention (1→2 and 2→1)\n",
    "    # ---------------------------------------------------\n",
    "    c1 = cross_attention_block(x1, x2, x2, mask2, heads, d_dim)\n",
    "    c2 = cross_attention_block(x2, x1, x1, mask1, heads, d_dim)\n",
    "\n",
    "    # Add residual (same as PyTorch + skip)\n",
    "    sc1 = layers.Add()([s1, c1])\n",
    "    sc2 = layers.Add()([s2, c2])\n",
    "\n",
    "    sc1 = layers.Dropout(drop_pool)(sc1)\n",
    "    sc2 = layers.Dropout(drop_pool)(sc2)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 4) Hybrid Pooling (max + mean)\n",
    "    # ---------------------------------------------------\n",
    "    h1 = HybridPooling()(sc1)\n",
    "    h2 = HybridPooling()(sc2)\n",
    "\n",
    "    merged = layers.Concatenate()([h1, h2])\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # 5) MLP Head\n",
    "    # ---------------------------------------------------\n",
    "    x = layers.Dense(256, activation=\"relu\")(merged)\n",
    "    x = layers.Dropout(drop_linear)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(drop_linear)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs=[inp1, inp2, mask1, mask2], outputs=out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20dedf9-f403-4700-ae96-3c9d76c3ce79",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install wandb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71f803c-4ea6-4e15-979f-a3356400dfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07e1ce9-f9c4-44d8-a2bf-0a47d832cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weights/Human-LR_PPI-RELAAI-ProtT5-20260104-065453'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# DEFAULT CONFIG (Logged Automatically to W&B)\n",
    "# =========================================================\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": 4,\n",
    "    \"n_splits\": 5,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"architecture\": \"ProtT5\",\n",
    "    \"dataset\": \"Human-LR_PPI\",\n",
    "    \"task\": \"Prot-Prot Classification\",\n",
    "    \"random_state\": 42,\n",
    "    \"max_len\":512,\n",
    "}\n",
    "\n",
    "PROJECT_NAME = f\"{CONFIG['dataset']}-RELAAI-{CONFIG['architecture']}-{TIMESTAMP}\"\n",
    "OUT_PATH = os.path.join(\"weights\",PROJECT_NAME)\n",
    "os.makedirs(os.path.join(OUT_PATH, \"logs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_PATH, \"weights\"), exist_ok=True)\n",
    "OUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b2c7f-9ce5-42c2-a630-e24bb5bd97ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      " Fold 1 / 5\n",
      "==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhossainstudy7\u001b[0m (\u001b[33mhossainstudy7-freelancer\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mhossai5/Ag-Ab-Affinity2/Final-Exp-Dec-18-25/wandb/run-20260104_065500-4z32gve6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/4z32gve6' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/4z32gve6' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/4z32gve6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1767531309.304466    8582 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0\n",
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/callbacks/tensorboard.py:680: UserWarning: Model failed to serialize as JSON. Ignoring... 'LlamaTransformerDecoder' object has no attribute 'rope_scaling_factor'\n",
      "  warnings.warn(f\"Model failed to serialize as JSON. Ignoring... {exc}\")\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767531322.988159    9909 service.cc:152] XLA service 0x7ffe54001500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1767531322.988206    9909 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1767531324.627534    9909 cuda_dnn.cc:529] Loaded cuDNN version 90500\n",
      "I0000 00:00:1767531331.510152    9909 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8189 - auc: 0.8864 - loss: 0.4290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 40ms/step - accuracy: 0.9007 - auc: 0.9625 - loss: 0.2505 - val_accuracy: 0.9678 - val_auc: 0.9934 - val_loss: 0.1181\n",
      "Epoch 2/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 40ms/step - accuracy: 0.9695 - auc: 0.9923 - loss: 0.0985 - val_accuracy: 0.9680 - val_auc: 0.9945 - val_loss: 0.1322\n",
      "Epoch 3/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 39ms/step - accuracy: 0.9775 - auc: 0.9943 - loss: 0.0771 - val_accuracy: 0.9792 - val_auc: 0.9954 - val_loss: 0.0901\n",
      "Epoch 4/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 35ms/step - accuracy: 0.9808 - auc: 0.9954 - loss: 0.0664 - val_accuracy: 0.9847 - val_auc: 0.9964 - val_loss: 0.0831\n",
      "Epoch 5/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 41ms/step - accuracy: 0.9811 - auc: 0.9962 - loss: 0.0602 - val_accuracy: 0.9824 - val_auc: 0.9962 - val_loss: 0.0654\n",
      "Epoch 6/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 40ms/step - accuracy: 0.9839 - auc: 0.9969 - loss: 0.0537 - val_accuracy: 0.9837 - val_auc: 0.9961 - val_loss: 0.0732\n",
      "Epoch 7/10\n",
      "\u001b[1m13169/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9873 - auc: 0.9973 - loss: 0.0450Epoch 8/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 38ms/step - accuracy: 0.9869 - auc: 0.9976 - loss: 0.0435 - val_accuracy: 0.9852 - val_auc: 0.9960 - val_loss: 0.0584\n",
      "Epoch 9/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 36ms/step - accuracy: 0.9880 - auc: 0.9979 - loss: 0.0410 - val_accuracy: 0.9869 - val_auc: 0.9965 - val_loss: 0.0572\n",
      "Epoch 10/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 34ms/step - accuracy: 0.9890 - auc: 0.9979 - loss: 0.0371 - val_accuracy: 0.9859 - val_auc: 0.9966 - val_loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3290/3293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3293/3293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step\n",
      "{'fold': 1, 'accuracy': 0.985876993166287, 'f1': 0.9860799281544679, 'recall': 0.9800654567093127, 'precision': 0.9921686746987952, 'roc_auc': 0.9966459065931428, 'aupr': 0.9973000398625909, 'specificity': 0.9919354838709677}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>aupr</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>epoch/auc</td><td>▁▇▇▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▁▅▇▆▇█▇██</td></tr><tr><td>epoch/val_auc</td><td>▁▃▅█▇▇█▇██</td></tr><tr><td>epoch/val_loss</td><td>▇█▄▃▂▂▂▁▁▁</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.98588</td></tr><tr><td>aupr</td><td>0.9973</td></tr><tr><td>epoch/accuracy</td><td>0.98903</td></tr><tr><td>epoch/auc</td><td>0.99794</td></tr><tr><td>epoch/epoch</td><td>9</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.03712</td></tr><tr><td>epoch/val_accuracy</td><td>0.98588</td></tr><tr><td>epoch/val_auc</td><td>0.99658</td></tr><tr><td>epoch/val_loss</td><td>0.05954</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/4z32gve6' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/4z32gve6</a><br> View project at: <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260104_065500-4z32gve6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      " Fold 2 / 5\n",
      "==========================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mhossai5/Ag-Ab-Affinity2/Final-Exp-Dec-18-25/wandb/run-20260104_082003-it4fepi1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/it4fepi1' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/it4fepi1' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/it4fepi1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/callbacks/tensorboard.py:680: UserWarning: Model failed to serialize as JSON. Ignoring... 'LlamaTransformerDecoder' object has no attribute 'rope_scaling_factor'\n",
      "  warnings.warn(f\"Model failed to serialize as JSON. Ignoring... {exc}\")\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8241 - auc: 0.8911 - loss: 0.4146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 35ms/step - accuracy: 0.9018 - auc: 0.9621 - loss: 0.2511 - val_accuracy: 0.9758 - val_auc: 0.9949 - val_loss: 0.1291\n",
      "Epoch 2/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 34ms/step - accuracy: 0.9680 - auc: 0.9912 - loss: 0.1039 - val_accuracy: 0.9774 - val_auc: 0.9965 - val_loss: 0.1015\n",
      "Epoch 3/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 34ms/step - accuracy: 0.9766 - auc: 0.9940 - loss: 0.0796 - val_accuracy: 0.9815 - val_auc: 0.9970 - val_loss: 0.0838\n",
      "Epoch 4/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 34ms/step - accuracy: 0.9801 - auc: 0.9949 - loss: 0.0691 - val_accuracy: 0.9838 - val_auc: 0.9970 - val_loss: 0.0878\n",
      "Epoch 5/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 34ms/step - accuracy: 0.9822 - auc: 0.9959 - loss: 0.0608 - val_accuracy: 0.9850 - val_auc: 0.9977 - val_loss: 0.0922\n",
      "Epoch 6/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 34ms/step - accuracy: 0.9837 - auc: 0.9966 - loss: 0.0557 - val_accuracy: 0.9836 - val_auc: 0.9977 - val_loss: 0.0732\n",
      "Epoch 7/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 34ms/step - accuracy: 0.9856 - auc: 0.9969 - loss: 0.0512 - val_accuracy: 0.9868 - val_auc: 0.9979 - val_loss: 0.0605\n",
      "Epoch 8/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 34ms/step - accuracy: 0.9864 - auc: 0.9972 - loss: 0.0468 - val_accuracy: 0.9860 - val_auc: 0.9977 - val_loss: 0.0536\n",
      "Epoch 9/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 34ms/step - accuracy: 0.9875 - auc: 0.9976 - loss: 0.0425 - val_accuracy: 0.9865 - val_auc: 0.9976 - val_loss: 0.0480\n",
      "Epoch 10/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 34ms/step - accuracy: 0.9883 - auc: 0.9980 - loss: 0.0386 - val_accuracy: 0.9860 - val_auc: 0.9976 - val_loss: 0.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3290/3293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3293/3293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step\n",
      "{'fold': 2, 'accuracy': 0.9859529233105543, 'f1': 0.9862627162693993, 'recall': 0.9879500148765249, 'precision': 0.9845811712379541, 'roc_auc': 0.9977245238589187, 'aupr': 0.9978156767706459, 'specificity': 0.9838709677419355}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>aupr</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>epoch/auc</td><td>▁▇▇▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▅▆▇▆███▇</td></tr><tr><td>epoch/val_auc</td><td>▁▅▆▆████▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▄▄▅▃▂▂▁▁</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.98595</td></tr><tr><td>aupr</td><td>0.99782</td></tr><tr><td>epoch/accuracy</td><td>0.98833</td></tr><tr><td>epoch/auc</td><td>0.99801</td></tr><tr><td>epoch/epoch</td><td>9</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.03862</td></tr><tr><td>epoch/val_accuracy</td><td>0.98595</td></tr><tr><td>epoch/val_auc</td><td>0.99759</td></tr><tr><td>epoch/val_loss</td><td>0.04713</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_2</strong> at: <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/it4fepi1' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/it4fepi1</a><br> View project at: <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260104_082003-it4fepi1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      " Fold 3 / 5\n",
      "==========================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mhossai5/Ag-Ab-Affinity2/Final-Exp-Dec-18-25/wandb/run-20260104_093717-6p1yvbp4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/6p1yvbp4' target=\"_blank\">fold_3</a></strong> to <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/6p1yvbp4' target=\"_blank\">https://wandb.ai/hossainstudy7-freelancer/Human-LR_PPI-RELAAI-ProtT5-20260104-065453/runs/6p1yvbp4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/home/mhossai5/.conda/envs/antibody_dl/lib/python3.10/site-packages/keras/src/callbacks/tensorboard.py:680: UserWarning: Model failed to serialize as JSON. Ignoring... 'LlamaTransformerDecoder' object has no attribute 'rope_scaling_factor'\n",
      "  warnings.warn(f\"Model failed to serialize as JSON. Ignoring... {exc}\")\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n",
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8083 - auc: 0.8756 - loss: 0.4429"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are explicitly setting `padding_mask` while the `inputs` have built-in mask, so the built-in mask is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 36ms/step - accuracy: 0.8949 - auc: 0.9584 - loss: 0.2639 - val_accuracy: 0.9766 - val_auc: 0.9941 - val_loss: 0.1356\n",
      "Epoch 2/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 35ms/step - accuracy: 0.9689 - auc: 0.9917 - loss: 0.1021 - val_accuracy: 0.9835 - val_auc: 0.9948 - val_loss: 0.0722\n",
      "Epoch 3/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 35ms/step - accuracy: 0.9778 - auc: 0.9945 - loss: 0.0762 - val_accuracy: 0.9819 - val_auc: 0.9954 - val_loss: 0.0859\n",
      "Epoch 4/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 34ms/step - accuracy: 0.9809 - auc: 0.9955 - loss: 0.0654 - val_accuracy: 0.9767 - val_auc: 0.9959 - val_loss: 0.1066\n",
      "Epoch 5/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 34ms/step - accuracy: 0.9835 - auc: 0.9965 - loss: 0.0572 - val_accuracy: 0.9814 - val_auc: 0.9963 - val_loss: 0.0754\n",
      "Epoch 6/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 34ms/step - accuracy: 0.9845 - auc: 0.9965 - loss: 0.0538 - val_accuracy: 0.9853 - val_auc: 0.9961 - val_loss: 0.0623\n",
      "Epoch 7/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 35ms/step - accuracy: 0.9861 - auc: 0.9972 - loss: 0.0472 - val_accuracy: 0.9847 - val_auc: 0.9959 - val_loss: 0.0595\n",
      "Epoch 8/10\n",
      "\u001b[1m13170/13170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 34ms/step - accuracy: 0.9869 - auc: 0.9974 - loss: 0.0449 - val_accuracy: 0.9870 - val_auc: 0.9968 - val_loss: 0.0496\n",
      "Epoch 9/10\n",
      "\u001b[1m  901/13170\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:03\u001b[0m 30ms/step - accuracy: 0.9931 - auc: 0.9986 - loss: 0.0262"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    roc_auc_score, matthews_corrcoef, confusion_matrix,\n",
    "    average_precision_score\n",
    ")\n",
    "CONFIG['n_splits'] =5\n",
    "# =============================================================\n",
    "#  K-FOLD CROSS VALIDATION SETTINGS\n",
    "# =============================================================\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_splits'], shuffle=True, random_state=42)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# =============================================================\n",
    "#  MAIN LOOP\n",
    "# =============================================================\n",
    "for fold,(train_idx, valid_idx) in enumerate(skf.split(df, df[\"label\"]),1):\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\" Fold {fold} / {CONFIG['n_splits']}\")\n",
    "    print(f\"==========================\")\n",
    "    run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            name=f\"fold_{fold}\",\n",
    "            group=\"KFold-CV\",\n",
    "            config={**CONFIG, \"fold\": fold},\n",
    "            reinit=True\n",
    "        )\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = df.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Loaders\n",
    "    # ---------------------------------------\n",
    "    train_loader = DataSequenceLoader(\n",
    "        train_df, \n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        max_len = CONFIG['max_len'], \n",
    "        shuffle=True)\n",
    "    valid_loader = DataSequenceLoader(\n",
    "        valid_df, \n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        max_len = CONFIG['max_len'], \n",
    "        shuffle=False)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Build a FRESH MODEL per fold\n",
    "    # ---------------------------------------\n",
    "    model = build_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(CONFIG[\"learning_rate\"]),\n",
    "        loss=CONFIG[\"loss\"],\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(name=\"auc\")\n",
    "        ]\n",
    "    )\n",
    "    # -----------------------------------------------------\n",
    "    # Callbacks\n",
    "    # -----------------------------------------------------\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(OUT_PATH, \"logs\", f\"fold_{fold}\")\n",
    "    )\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\n",
    "            OUT_PATH, \"weights\",\n",
    "            f\"weights_fold{fold}-best.weights.h5\"\n",
    "        ),\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "    # ---------------------------------------\n",
    "    # Train\n",
    "    # ---------------------------------------\n",
    "    history = model.fit(\n",
    "        train_loader,\n",
    "        validation_data=valid_loader,\n",
    "        epochs=CONFIG[\"epochs\"],\n",
    "        callbacks=[\n",
    "            tb_callback,\n",
    "            checkpoint_cb,\n",
    "            WandbMetricsLogger(log_freq=\"epoch\")\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save_weights(\n",
    "        os.path.join(\n",
    "            OUT_PATH,\n",
    "            \"weights\",\n",
    "            f\"weights_fold{fold}-last.weights.h5\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Evaluation\n",
    "    # -----------------------------------------------------\n",
    "    y_pred_prob = model.predict(valid_loader).ravel()\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    y_true = valid_df[\"label\"].values\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"fold\": fold,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_pred_prob),\n",
    "        \"aupr\": average_precision_score(y_true, y_pred_prob),\n",
    "        \"specificity\": specificity\n",
    "    }\n",
    "\n",
    "    # Log fold metrics to W&B\n",
    "    wandb.log(metrics_dict)\n",
    "\n",
    "    print(metrics_dict)\n",
    "    all_metrics.append(metrics_dict)\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "# =========================================================\n",
    "# Save All Metrics\n",
    "# =========================================================\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "avg_row = metrics_df.mean(numeric_only=True)\n",
    "metrics_df = pd.concat(\n",
    "    [metrics_df, avg_row.to_frame().T],\n",
    "    ignore_index=True\n",
    ")\n",
    "metrics_df.loc[metrics_df.index[-1], \"fold\"] = \"Average\"\n",
    "\n",
    "metrics_df.to_csv(\n",
    "    os.path.join(\n",
    "        OUT_PATH,\n",
    "        f\"{PROJECT_NAME}-kfold_classification_metrics.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nAll fold metrics saved.\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5ad47-1e4b-4cb8-95ac-1bf6e54766c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "#  EVALUATION (POST-TRAINING)\n",
    "# =============================================================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    roc_auc_score, matthews_corrcoef,\n",
    "    confusion_matrix, average_precision_score\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_splits'], shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (_, valid_idx) in enumerate(\n",
    "        skf.split(df, df[\"Interaction\"]), 1):\n",
    "\n",
    "    print(f\"\\nEvaluating Fold {fold}\")\n",
    "\n",
    "    valid_df = df.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_loader = DataSequenceLoader(\n",
    "        valid_df,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        max_len=CONFIG[\"max_len\"],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Rebuild model & load best weights\n",
    "    model = build_model()\n",
    "    model.load_weights(\n",
    "        os.path.join(\n",
    "            OUT_PATH, \"weights\",\n",
    "            f\"weights_fold{fold}-best.weights.h5\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Prediction\n",
    "    y_prob = model.predict(valid_loader).ravel()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    y_true = valid_df[\"label\"].values\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"fold\": fold,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),     # sensitivity\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "        \"auc\": roc_auc_score(y_true, y_prob),\n",
    "        \"prauc\": average_precision_score(y_true, y_prob),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
    "        \"sensitivity\": tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "    print(metrics_dict)\n",
    "    all_metrics.append(metrics_dict)\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "avg_row = metrics_df.mean(numeric_only=True)\n",
    "avg_row[\"fold\"] = \"Average\"\n",
    "\n",
    "metrics_df = pd.concat(\n",
    "    [metrics_df, avg_row.to_frame().T],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "metrics_df.to_csv(\n",
    "    os.path.join(\n",
    "        OUT_PATH,\n",
    "        f\"{PROJECT_NAME}-kfold_evaluation_metrics.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation completed and saved.\")\n",
    "metrics_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-antibody_dl]",
   "language": "python",
   "name": "conda-env-.conda-antibody_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
